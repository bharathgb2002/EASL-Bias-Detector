{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "0xflSCns2u6i",
        "outputId": "5cd0c913-4b1a-40b2-97ad-a1794a504619"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://2738a5a9664cdbdbca.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://2738a5a9664cdbdbca.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "!pip install -q gradio scikit-learn pandas matplotlib pillow textblob\n",
        "!pip install -q wikipedia-api\n",
        "\n",
        "import os, csv, json, io, re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "from datetime import datetime\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "# REMOVE this bad line:  import wikipediaapi,\n",
        "# REMOVE if unused: from urllib.parse import quote\n",
        "\n",
        "# --- Wikipedia API setup (for fact-checking) ---\n",
        "try:\n",
        "    import wikipediaapi\n",
        "    _wiki = wikipediaapi.Wikipedia('en')\n",
        "except Exception:\n",
        "    wikipediaapi = None\n",
        "    _wiki = None\n",
        "\n",
        "\n",
        "# ---------------- Optional: sentiment to help opinion rule ----------------\n",
        "try:\n",
        "    from textblob import TextBlob\n",
        "    _HAS_TB = True\n",
        "except Exception:\n",
        "    _HAS_TB = False\n",
        "\n",
        "# ============== helper: return current Matplotlib figure as a PIL image ==============\n",
        "def _fig_to_pil():\n",
        "    buf = io.BytesIO()\n",
        "    plt.savefig(buf, format=\"png\", dpi=150, bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "    buf.seek(0)\n",
        "    return Image.open(buf).convert(\"RGB\")\n",
        "\n",
        "# ===================================== Config =====================================\n",
        "CSV_HEADERS = [\n",
        "    \"timestamp\", \"trainer\", \"country\",            # country included\n",
        "    \"question\", \"sentence\", \"model_source\",\n",
        "    \"predicted_bias_pct\", \"predicted_label\", \"predicted_category\",\n",
        "    \"confidence\", \"rule_explanation\", \"true_category\",\n",
        "    \"corrected_binary_label\", \"correct_binary\", \"correct_category\",\n",
        "    \"is_question\", \"predict_method\"              # ML or RULE for traceability\n",
        "]\n",
        "TEAM = [\"Gokulan\",\"Nathan\",\"Bharath\",\"Gabel Nibu\",\"Keerthi\",\"Krishnalaya\",\"others\"]\n",
        "\n",
        "CATEGORIES = [\"Neutral\",\"Gender\",\"Cultural\",\"Political\",\"Other\"]\n",
        "CAT2ID = {k.lower(): i for i, k in enumerate(CATEGORIES)}\n",
        "ID2CAT = {v: k for k, v in CAT2ID.items()}\n",
        "ALL_CLASSES = np.array(sorted(CAT2ID.values()))\n",
        "TRAIN_LOG_PATH = \"/content/trained_prompts_mc.csv\"  # change if needed\n",
        "\n",
        "# ===================================== Utilities =====================================\n",
        "def is_question_text(text:str)->bool:\n",
        "    t = str(text).strip().lower()\n",
        "    return t.endswith(\"?\") or t.startswith((\"why\",\"what\",\"how\",\"who\",\"when\",\"where\",\"which\"))\n",
        "\n",
        "def label_from_pct(pct:int)->str:\n",
        "    return \"Biased\" if pct >= 50 else \"Not Biased\"\n",
        "\n",
        "def corrected_binary(true_cat:str)->str:\n",
        "    return \"Not Biased\" if str(true_cat).lower()==\"neutral\" else \"Biased\"\n",
        "\n",
        "# ===================== RULE ENGINE: opinion vs information =====================\n",
        "OPINION_PHRASES = [\n",
        "    \"i think\", \"i believe\", \"in my opinion\", \"we think\", \"i feel\", \"it seems\", \"i guess\",\n",
        "    \"should\", \"must\", \"ought to\", \"need to\", \"clearly\", \"obviously\", \"undeniably\",\n",
        "    \"best\", \"worst\", \"superior\", \"inferior\", \"terrible\", \"amazing\", \"disgraceful\",\n",
        "    \"always\", \"never\", \"everyone knows\", \"no doubt\"\n",
        "]\n",
        "COMPARATIVES_SUPERLATIVES = [r\"\\w+er\\b\", r\"\\bmore\\b\", r\"\\bless\\b\", r\"\\w+est\\b\"]\n",
        "FIRST_SECOND_PERSON = [r\"\\bi\\b\", r\"\\bwe\\b\", r\"\\byou\\b\", r\"\\bours\\b\", r\"\\bmy\\b\", r\"\\bme\\b\"]\n",
        "MODALS = [r\"\\bshould\\b\", r\"\\bmust\\b\", r\"\\bneed to\\b\", r\"\\bought to\\b\", r\"\\bhas to\\b\", r\"\\bhave to\\b\"]\n",
        "EXAGGERATION = [r\"\\balways\\b\", r\"\\bnever\\b\", r\"\\beveryone\\b\", r\"\\bnobody\\b\"]\n",
        "EVALUATIVE_ADJ = [\"great\", \"awful\", \"brilliant\", \"stupid\", \"pathetic\", \"heroic\", \"shameful\", \"useless\"]\n",
        "\n",
        "INFO_CUES_PHRASES = [\n",
        "    \"according to\", \"reported\", \"announced\", \"data show\", \"the report\", \"official\",\n",
        "    \"the study\", \"statistics\", \"percent\", \"%\", \"figures\", \"survey\", \"stated\", \"said\"\n",
        "]\n",
        "COPULA_NEUTRAL = [r\"\\bis\\b\", r\"\\bare\\b\", r\"\\bwas\\b\", r\"\\bwere\\b\"]\n",
        "NUMERIC_PAT = [r\"\\b\\d{1,2}(:\\d{2})?\\b\", r\"\\b\\d{4}\\b\", r\"\\b\\d+(\\.\\d+)?\\b\"]\n",
        "\n",
        "KEYWORDS_TOPIC = {\n",
        "    \"Gender\": [\"men\",\"women\",\"female\",\"male\",\"gender\",\"sexism\",\"feminist\",\"patriarchy\"],\n",
        "    \"Political\": [\"election\",\"government\",\"minister\",\"policy\",\"party\",\"parliament\",\"modi\",\"trump\",\"bjp\",\"congress\"],\n",
        "    \"Cultural\": [\"religion\",\"hindu\",\"muslim\",\"christian\",\"culture\",\"tradition\",\"festival\",\"ethnicity\",\"language\"],\n",
        "    \"Other\": []\n",
        "}\n",
        "\n",
        "def _count_hits(patterns, tl):\n",
        "    count = 0\n",
        "    for p in patterns:\n",
        "        if p.startswith(\"\\\\b\") or \"[\" in p or p.endswith(\"\\\\b\"):\n",
        "            count += len(re.findall(p, tl))\n",
        "        else:\n",
        "            count += tl.count(p)\n",
        "    return count\n",
        "\n",
        "def _sentiment_strength(text):\n",
        "    if not _HAS_TB:\n",
        "        return 0.0\n",
        "    s = TextBlob(text).sentiment.polarity\n",
        "    return abs(float(s))\n",
        "\n",
        "def opinion_info_scores(text: str):\n",
        "    t = (text or \"\").strip()\n",
        "    tl = t.lower()\n",
        "\n",
        "    opinion_hits = 0\n",
        "    opinion_hits += _count_hits([re.escape(p) for p in OPINION_PHRASES], tl)\n",
        "    opinion_hits += _count_hits(COMPARATIVES_SUPERLATIVES, tl)\n",
        "    opinion_hits += _count_hits(FIRST_SECOND_PERSON, tl)\n",
        "    opinion_hits += _count_hits(MODALS, tl)\n",
        "    opinion_hits += _count_hits(EXAGGERATION, tl)\n",
        "    opinion_hits += sum(1 for w in EVALUATIVE_ADJ if re.search(rf\"\\b{re.escape(w)}\\b\", tl))\n",
        "\n",
        "    info_hits = 0\n",
        "    info_hits += _count_hits([re.escape(p) for p in INFO_CUES_PHRASES], tl)\n",
        "    info_hits += _count_hits(COPULA_NEUTRAL, tl)\n",
        "    info_hits += _count_hits(NUMERIC_PAT, tl)\n",
        "\n",
        "    opinion_strength = opinion_hits + 2.0 * _sentiment_strength(t)\n",
        "    return opinion_strength, float(info_hits)\n",
        "\n",
        "def topic_category_hint(text):\n",
        "    tl = (text or \"\").lower()\n",
        "    best_cat, best_hits = \"Neutral\", 0\n",
        "    for cat, words in KEYWORDS_TOPIC.items():\n",
        "        hits = sum(1 for w in words if re.search(rf\"\\b{re.escape(w)}\\b\", tl))\n",
        "        if hits > best_hits:\n",
        "            best_hits, best_cat = hits, cat\n",
        "    return best_cat if best_hits>0 else \"Neutral\"\n",
        "\n",
        "def opinion_bias_rule(text: str):\n",
        "    op, info = opinion_info_scores(text)\n",
        "    margin = op - info  # positive => opinion-ish\n",
        "    if margin <= -1.0:\n",
        "        return 0, \"Informational tone (facts/citations outnumber opinion cues).\"\n",
        "    if -1.0 < margin < 1.0:\n",
        "        return 25, \"Mixed tone (both factual and opinion cues present).\"\n",
        "    if 1.0 <= margin < 3.0:\n",
        "        return 60, \"Opinionated tone (recommendations/evaluatives or personal stance).\"\n",
        "    return 85, \"Strong opinion/persuasive language (modals/evaluatives/pronouns/high sentiment).\"\n",
        "\n",
        "def combine_opinion_and_fact_bias(text: str):\n",
        "    \"\"\"\n",
        "    Uses your opinion rule to get a base bias%, then adjusts with a Wikipedia fact score.\n",
        "    - High fact score + low opinion => push toward Not Biased\n",
        "    - Low fact score + high opinion => push toward Biased\n",
        "    \"\"\"\n",
        "    base_bias, rationale_rule = opinion_bias_rule(text)  # 0..100\n",
        "    fact_sc, fact_note = wiki_fact_score(text)           # 0..1\n",
        "\n",
        "    # Map fact score to a +/- adjustment\n",
        "    # Intuition: 0.0 => +15 (more biased), 1.0 => -25 (less biased)\n",
        "    # Keep it conservative and bounded\n",
        "    adj = int(round((-25 * fact_sc) + (15 * (1.0 - fact_sc)) * (1 if base_bias >= 50 else 0.5)))\n",
        "    new_bias = int(np.clip(base_bias + adj, 0, 100))\n",
        "\n",
        "    note = f\"{rationale_rule} | {fact_note}\"\n",
        "    return new_bias, note, fact_sc\n",
        "\n",
        "# ---------- Simple topic extraction for Wikipedia lookups ----------\n",
        "def extract_topics_for_wiki(text: str, max_topics: int = 3):\n",
        "    \"\"\"\n",
        "    Very light entity/topic guesser:\n",
        "    - Capitalized multiword phrases (e.g., \"Narendra Modi\", \"United States\")\n",
        "    - Fallback to top non-stopword nouns-ish words\n",
        "    \"\"\"\n",
        "    t = (text or \"\").strip()\n",
        "    # Grab sequences of capitalized words: e.g. \"New York City\"\n",
        "    caps = re.findall(r\"(?:[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)+)\", t)\n",
        "    # Single capitalized tokens that look like named entities\n",
        "    singles = re.findall(r\"\\b([A-Z][a-z]{2,})\\b\", t)\n",
        "    # Keep order of appearance, dedupe\n",
        "    seen = set()\n",
        "    topics = []\n",
        "    for s in caps + singles:\n",
        "        s = s.strip()\n",
        "        if s and s not in seen:\n",
        "            seen.add(s)\n",
        "            topics.append(s)\n",
        "        if len(topics) >= max_topics:\n",
        "            break\n",
        "    return topics or []\n",
        "\n",
        "# ---------- Wikipedia light fact-check ----------\n",
        "\n",
        "\n",
        "def _wiki_summary(topic: str, max_chars: int = 1200):\n",
        "    try:\n",
        "        if _wiki is None:\n",
        "            return \"\"\n",
        "        page = _wiki.page(topic)\n",
        "        if page.exists():\n",
        "            s = page.summary or \"\"\n",
        "            return s[:max_chars]\n",
        "    except Exception:\n",
        "        pass\n",
        "    return \"\"\n",
        "\n",
        "def _normalize_tokens(s: str):\n",
        "    s = s.lower()\n",
        "    # keep letters/numbers and spaces\n",
        "    s = re.sub(r\"[^a-z0-9\\s]\", \" \", s)\n",
        "    toks = [w for w in s.split() if len(w) > 2 and w not in {\n",
        "        \"the\",\"and\",\"for\",\"with\",\"that\",\"this\",\"from\",\"into\",\"were\",\"have\",\"has\",\"had\",\n",
        "        \"are\",\"was\",\"will\",\"would\",\"could\",\"should\",\"might\",\"about\",\"over\",\"under\",\n",
        "        \"there\",\"their\",\"them\",\"they\",\"you\",\"your\",\"our\",\"his\",\"her\",\"its\",\"not\"\n",
        "    }]\n",
        "    return set(toks)\n",
        "\n",
        "def jaccard(a: set, b: set):\n",
        "    if not a or not b: return 0.0\n",
        "    inter = len(a & b); uni = len(a | b)\n",
        "    return inter / uni if uni else 0.0\n",
        "\n",
        "def wiki_fact_score(text: str) -> tuple[float, str]:\n",
        "    \"\"\"\n",
        "    Returns (score [0..1], note).\n",
        "    Heuristic: max Jaccard between the sentence tokens and any matching wiki summary.\n",
        "    \"\"\"\n",
        "    # Safe guard if wikipediaapi isn't available / didn't import\n",
        "    if _wiki is None:\n",
        "        return 0.0, \"Wikipedia client not available.\"\n",
        "\n",
        "    topics = extract_topics_for_wiki(text, max_topics=3)\n",
        "    if not topics:\n",
        "        return 0.0, \"No topic found for wiki check.\"\n",
        "\n",
        "    sent_tokens = _normalize_tokens(text)\n",
        "    best = 0.0\n",
        "    best_topic = None\n",
        "\n",
        "    for tp in topics:\n",
        "        summ = _wiki_summary(tp)\n",
        "        if not summ:\n",
        "            continue\n",
        "        sc = jaccard(sent_tokens, _normalize_tokens(summ))\n",
        "        if sc > best:\n",
        "            best, best_topic = sc, tp\n",
        "\n",
        "    if best_topic:\n",
        "        return float(best), f\"Wikipedia match: {best_topic} (similarity={best:.2f})\"\n",
        "    return 0.0, \"No useful Wikipedia match.\"\n",
        "\n",
        "# ===================== Online ML =====================\n",
        "class OnlineBiasLearnerMC:\n",
        "    def __init__(self, n_features=2**18, random_state=42):\n",
        "        self.v = HashingVectorizer(n_features=n_features, alternate_sign=False, ngram_range=(1,2))\n",
        "        self.clf = SGDClassifier(loss=\"log_loss\", alpha=1e-5, random_state=random_state)\n",
        "        self.init = False\n",
        "    def _X(self, texts):\n",
        "        return self.v.transform(texts)\n",
        "    def partial_fit(self, texts, y):\n",
        "        X = self._X(texts)\n",
        "        if not self.init:\n",
        "            self.clf.partial_fit(X, np.array(y), classes=ALL_CLASSES)\n",
        "            self.init = True\n",
        "        else:\n",
        "            self.clf.partial_fit(X, np.array(y))\n",
        "        return self\n",
        "    def predict_proba(self, texts):\n",
        "        check_is_fitted(self.clf)\n",
        "        return self.clf.predict_proba(self._X(texts))\n",
        "\n",
        "# ===================== CSV helpers =====================\n",
        "def ensure_csv():\n",
        "    if not os.path.exists(TRAIN_LOG_PATH):\n",
        "        with open(TRAIN_LOG_PATH,\"w\",newline=\"\",encoding=\"utf-8\") as f:\n",
        "            csv.writer(f).writerow(CSV_HEADERS)\n",
        "\n",
        "def read_log_df():\n",
        "    ensure_csv()\n",
        "    try:\n",
        "        df = pd.read_csv(TRAIN_LOG_PATH)\n",
        "    except Exception:\n",
        "        with open(TRAIN_LOG_PATH,\"w\",newline=\"\",encoding=\"utf-8\") as f:\n",
        "            csv.writer(f).writerow(CSV_HEADERS)\n",
        "        df = pd.read_csv(TRAIN_LOG_PATH)\n",
        "    # forward-compat: add missing columns (e.g., country)\n",
        "    for c in CSV_HEADERS:\n",
        "        if c not in df.columns:\n",
        "            if c == \"is_question\":\n",
        "                df[c] = False\n",
        "            else:\n",
        "                df[c] = \"\"\n",
        "    df = df[CSV_HEADERS]\n",
        "    return df\n",
        "\n",
        "def append_row(row:list):\n",
        "    ensure_csv()\n",
        "    with open(TRAIN_LOG_PATH,\"a\",newline=\"\",encoding=\"utf-8\") as f:\n",
        "        csv.writer(f).writerow(row)\n",
        "\n",
        "def export_csv_copy():\n",
        "    ensure_csv()\n",
        "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    out = f\"/content/trained_prompts_mc_{ts}.csv\"\n",
        "    df = read_log_df()\n",
        "    df.to_csv(out, index=False)\n",
        "    return out\n",
        "\n",
        "def import_csv(filepath:str, mode:str):\n",
        "    base = read_log_df()\n",
        "    try:\n",
        "        inc = pd.read_csv(filepath)\n",
        "    except Exception as e:\n",
        "        return f\"⚠️ Failed to read CSV: {e}\", base\n",
        "    for c in CSV_HEADERS:\n",
        "        if c not in inc.columns:\n",
        "            inc[c] = \"\" if c not in (\"correct_binary\",\"correct_category\",\"is_question\") else (False if c==\"is_question\" else \"\")\n",
        "    inc = inc[CSV_HEADERS]\n",
        "    if mode==\"replace\":\n",
        "        inc.to_csv(TRAIN_LOG_PATH,index=False)\n",
        "        df, msg = inc, \"✅ Replaced log with uploaded CSV.\"\n",
        "    else:\n",
        "        df = pd.concat([base,inc],ignore_index=True)\n",
        "        df.to_csv(TRAIN_LOG_PATH,index=False)\n",
        "        msg = \"✅ Merged uploaded CSV into log.\"\n",
        "    global learner\n",
        "    learner = replay_from_csv()\n",
        "    return msg, df\n",
        "\n",
        "def replay_from_csv():\n",
        "    df = read_log_df()\n",
        "    m = OnlineBiasLearnerMC()\n",
        "    if df.empty:\n",
        "        return m\n",
        "    for _,r in df.iterrows():\n",
        "        txt = str(r.get(\"sentence\",\"\")); tc = str(r.get(\"true_category\",\"\")).lower()\n",
        "        if txt and tc in CAT2ID:\n",
        "            m.partial_fit([txt],[CAT2ID[tc]])\n",
        "    return m\n",
        "\n",
        "# ===================== PCA helpers =====================\n",
        "def create_pca_plot():\n",
        "    df = read_log_df()\n",
        "    if df is None or df.empty:\n",
        "        plt.figure(figsize=(4,2)); plt.text(0.5,0.5,\"No data in log\",ha=\"center\",va=\"center\"); plt.axis(\"off\")\n",
        "        return _fig_to_pil(), \"⚠️ No training data yet — the log is empty.\"\n",
        "\n",
        "    if \"sentence\" not in df.columns:\n",
        "        plt.figure(figsize=(4,2)); plt.text(0.5,0.5,\"Missing 'sentence' column\",ha=\"center\",va=\"center\"); plt.axis(\"off\")\n",
        "        return _fig_to_pil(), \"⚠️ Log is missing the 'sentence' column.\"\n",
        "\n",
        "    texts = df[\"sentence\"].astype(str)\n",
        "\n",
        "    # Prefer human labels if present; otherwise predicted; else Neutral\n",
        "    if \"true_category\" in df.columns and df[\"true_category\"].notna().any():\n",
        "        labels = df[\"true_category\"].fillna(\"Neutral\").astype(str)\n",
        "    elif \"predicted_category\" in df.columns and df[\"predicted_category\"].notna().any():\n",
        "        labels = df[\"predicted_category\"].fillna(\"Neutral\").astype(str)\n",
        "    else:\n",
        "        labels = pd.Series([\"Neutral\"] * len(df))\n",
        "\n",
        "    if texts.str.len().sum() == 0:\n",
        "        plt.figure(figsize=(4,2)); plt.text(0.5,0.5,\"No text to plot\",ha=\"center\",va=\"center\"); plt.axis(\"off\")\n",
        "        return _fig_to_pil(), \"⚠️ No text to plot.\"\n",
        "\n",
        "    vectorizer = HashingVectorizer(n_features=2**12, alternate_sign=False, ngram_range=(1, 2))\n",
        "    X = vectorizer.transform(texts).toarray()\n",
        "    if X.shape[0] < 2:\n",
        "        plt.figure(figsize=(4,2)); plt.text(0.5,0.5,\"Need ≥ 2 samples for PCA\",ha=\"center\",va=\"center\"); plt.axis(\"off\")\n",
        "        return _fig_to_pil(), \"⚠️ Need at least 2 samples for PCA.\"\n",
        "\n",
        "    pca = PCA(n_components=2, random_state=42)\n",
        "    X2 = pca.fit_transform(X)\n",
        "\n",
        "    plt.figure(figsize=(10,7))\n",
        "    cats = labels.unique()\n",
        "    cmap = plt.cm.get_cmap(\"tab10\", len(cats))\n",
        "    for i, cat in enumerate(cats):\n",
        "        mask = (labels == cat).to_numpy()\n",
        "        plt.scatter(X2[mask,0], X2[mask,1], label=cat, alpha=0.85, s=30, c=[cmap(i)])\n",
        "    plt.title(\"PCA of Sentences by Category\"); plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\n",
        "    plt.legend(title=\"Category\", bbox_to_anchor=(1.02,1), loc=\"upper left\")\n",
        "    plt.tight_layout()\n",
        "    return _fig_to_pil(), \"✅ PCA (by category) generated.\"\n",
        "\n",
        "def create_model_pca_plot():\n",
        "    df = read_log_df()\n",
        "    if df is None or df.empty:\n",
        "        plt.figure(figsize=(4, 2)); plt.text(0.5, 0.5, \"No data in log\", ha=\"center\", va=\"center\"); plt.axis(\"off\")\n",
        "        return _fig_to_pil(), \"⚠️ No training data yet — the log is empty.\"\n",
        "\n",
        "    missing = [c for c in [\"sentence\", \"model_source\"] if c not in df.columns]\n",
        "    if missing:\n",
        "        plt.figure(figsize=(4, 2)); plt.text(0.5, 0.5, f\"Missing: {', '.join(missing)}\", ha=\"center\", va=\"center\"); plt.axis(\"off\")\n",
        "        return _fig_to_pil(), f\"⚠️ Missing columns: {', '.join(missing)}.\"\n",
        "\n",
        "    texts  = df[\"sentence\"].astype(str)\n",
        "    models = df[\"model_source\"].astype(str)\n",
        "    bias_values = pd.to_numeric(df.get(\"predicted_bias_pct\", 0), errors=\"coerce\").fillna(0).clip(0, 100).values\n",
        "\n",
        "    vectorizer = HashingVectorizer(n_features=2**12, alternate_sign=False, ngram_range=(1, 2))\n",
        "    X = vectorizer.transform(texts).toarray()\n",
        "    if X.shape[0] < 2:\n",
        "        plt.figure(figsize=(4, 2)); plt.text(0.5, 0.5, \"Need ≥ 2 samples for PCA\", ha=\"center\", va=\"center\"); plt.axis(\"off\")\n",
        "        return _fig_to_pil(), \"⚠️ Need at least 2 samples for PCA.\"\n",
        "\n",
        "    pca = PCA(n_components=2, random_state=42)\n",
        "    X2 = pca.fit_transform(X)\n",
        "\n",
        "    color_map = {\"ChatGPT\": \"red\", \"Claude\": \"green\", \"Gemini\": \"blue\"}\n",
        "\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    for model in sorted(models.unique()):\n",
        "        mask = (models == model).to_numpy()\n",
        "        color = color_map.get(model, \"gray\")\n",
        "        sizes  = 12 + bias_values[mask] * 0.9\n",
        "        alphas = np.clip(0.35 + bias_values[mask] / 150.0, 0.35, 0.95)\n",
        "        plt.scatter(X2[mask, 0], X2[mask, 1], s=sizes, color=color, alpha=alphas, edgecolors=\"none\", label=model)\n",
        "\n",
        "    plt.title(\"PCA of Sentences by Model (color=model, size/opacity=bias%)\")\n",
        "    plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\n",
        "    plt.legend(title=\"Model\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
        "    plt.tight_layout()\n",
        "    return _fig_to_pil(), \"✅ PCA (by model) generated.\"\n",
        "\n",
        "def create_dual_pca_plots():\n",
        "    try:\n",
        "        img1, _ = create_pca_plot()\n",
        "    except Exception:\n",
        "        plt.figure(figsize=(4,2)); plt.text(0.5,0.5,\"Category PCA error\",ha=\"center\",va=\"center\"); plt.axis(\"off\")\n",
        "        img1 = _fig_to_pil()\n",
        "    try:\n",
        "        img2, _ = create_model_pca_plot()\n",
        "    except Exception:\n",
        "        plt.figure(figsize=(4,2)); plt.text(0.5,0.5,\"Model PCA error\",ha=\"center\",va=\"center\"); plt.axis(\"off\")\n",
        "        img2 = _fig_to_pil()\n",
        "    return img1, img2, \"✅ Generated both PCA plots.\"\n",
        "\n",
        "# ===================== Core predict/train (ML + RULE fallback) =====================\n",
        "learner = replay_from_csv()\n",
        "MIN_ML_CONF = 0.55  # if ML max proba below this, use rules for this sentence\n",
        "\n",
        "def ml_or_rule_bias_and_cat(text: str):\n",
        "    \"\"\"\n",
        "    Try ML first; if learner uninitialized or low confidence, fall back to\n",
        "    opinion-rule + Wikipedia fact signal.\n",
        "    Returns: (bias_pct, cat_id, conf, method, rationale)\n",
        "    \"\"\"\n",
        "    # --- Attempt ML first ---\n",
        "    try:\n",
        "        if learner.init:\n",
        "            proba = learner.predict_proba([text])[0]\n",
        "            conf_ml = float(np.max(proba))\n",
        "            cat_id_ml = int(np.argmax(proba))\n",
        "            p_neu = float(proba[CAT2ID[\"neutral\"]])\n",
        "            bias_ml = 0 if p_neu >= 0.80 else int(round((1.0 - p_neu) * 100))\n",
        "            if conf_ml >= MIN_ML_CONF:\n",
        "                return max(0, min(100, bias_ml)), cat_id_ml, conf_ml, \"ML\", \"\"\n",
        "    except Exception:\n",
        "        pass  # if ML fails, fall through to rules\n",
        "\n",
        "    # --- Fallback: rules + fact check ---\n",
        "    # Combine opinion-vs-info rule with Wikipedia similarity\n",
        "    bias_rule_adj, note, fact_sc = combine_opinion_and_fact_bias(text)\n",
        "\n",
        "    # Category hint: if it looks biased and we detect a topic, use it; else Neutral\n",
        "    topic_cat = topic_category_hint(text)\n",
        "    final_cat_name = topic_cat if (bias_rule_adj > 50 and topic_cat != \"Neutral\") else \"Neutral\"\n",
        "    cat_id = CAT2ID.get(final_cat_name.lower(), CAT2ID[\"neutral\"])\n",
        "\n",
        "    # Pseudo-confidence: higher when opinion vs info margin is larger or facts are strong\n",
        "    op, info = opinion_info_scores(text)\n",
        "    margin = abs(op - info)\n",
        "    conf_rule = max(0.45, min(0.95, 0.50 + 0.12 * margin + 0.20 * fact_sc))\n",
        "\n",
        "    rationale = note  # carries opinion-note + wiki-note\n",
        "    return int(bias_rule_adj), int(cat_id), float(conf_rule), \"RULE+FACT\", rationale\n",
        "\n",
        "def predict_one(question, sentence, model_source, country: str):\n",
        "    bias_pct, cat_id, conf, method, rationale = ml_or_rule_bias_and_cat(sentence)\n",
        "    pred_label = label_from_pct(bias_pct)\n",
        "    qflag = is_question_text(sentence)\n",
        "    return {\n",
        "        \"question\": question,\n",
        "        \"country\": country or \"\",\n",
        "        \"sentence\": sentence,\n",
        "        \"model_source\": model_source,\n",
        "        \"bias_pct\": int(bias_pct),\n",
        "        \"pred_label\": pred_label,\n",
        "        \"pred_cat_name\": ID2CAT[int(cat_id)],\n",
        "        \"confidence\": float(conf),\n",
        "        \"rule_expl\": rationale,\n",
        "        \"method\": method,\n",
        "        \"is_question\": bool(qflag)\n",
        "    }\n",
        "\n",
        "def run_predictions(trainer, country, question, cgpt_ans, cla_ans, gem_ans):\n",
        "    if not (question or \"\").strip():\n",
        "        return \"⚠️ Please enter a question.\", \"\", \"\", \"\", \"\", None, None, None\n",
        "\n",
        "    items = []\n",
        "    if (cgpt_ans or \"\").strip():\n",
        "        items.append(predict_one(question, cgpt_ans, \"ChatGPT\", country))\n",
        "    if (cla_ans or \"\").strip():\n",
        "        items.append(predict_one(question, cla_ans,  \"Claude\",  country))\n",
        "    if (gem_ans or \"\").strip():\n",
        "        items.append(predict_one(question, gem_ans,  \"Gemini\",  country))\n",
        "\n",
        "    if not items:\n",
        "        return \"⚠️ Please enter answers.\", \"\", \"\", \"\", \"\", None, None, None\n",
        "\n",
        "    df = pd.DataFrame(items)\n",
        "    avg_by_model = df.groupby(\"model_source\")[\"bias_pct\"].mean().round(1).to_dict()\n",
        "    max_bias = float(df[\"bias_pct\"].max()) if not df.empty else 0.0\n",
        "\n",
        "    header_md = f\"**Question:** {question}  \\n**Country:** {country or '(none)'}\\n\\n\"\n",
        "    header_md += \" | \".join([f\"{m}: {avg_by_model.get(m,0):.1f}%\" for m in ['ChatGPT','Claude','Gemini']])\n",
        "\n",
        "    if max_bias <= 5.0:\n",
        "        header_md += \"\\n\\n✅ **No model biased (all Neutral)**\"\n",
        "    else:\n",
        "        worst_model = max(avg_by_model.items(), key=lambda kv: kv[1])[0]\n",
        "        header_md += f\"\\n\\n🚨 **Most Biased → {worst_model} ({avg_by_model[worst_model]:.1f}%)**\"\n",
        "\n",
        "    def section_md(model):\n",
        "        sub = df[df[\"model_source\"] == model]\n",
        "        if sub.empty:\n",
        "            return f\"### {model}\\n(No answer)\"\n",
        "        row = sub.iloc[0]\n",
        "        tag = str(row.get(\"method\", \"\"))\n",
        "        expl = (\n",
        "            f\"\\n\\n_Rule note:_ {row.get('rule_expl','')}\"\n",
        "            if tag.startswith(\"RULE\") and row.get(\"rule_expl\")\n",
        "            else \"\"\n",
        "        )\n",
        "        return (\n",
        "            f\"### {model} ({tag})\\n“{row['sentence']}” → \"\n",
        "            f\"**{row['bias_pct']}%** ({row['pred_cat_name']}){expl}\"\n",
        "        )\n",
        "\n",
        "    return (\n",
        "        header_md,\n",
        "        section_md(\"ChatGPT\"),\n",
        "        section_md(\"Claude\"),\n",
        "        section_md(\"Gemini\"),\n",
        "        json.dumps(items),\n",
        "        None, None, None\n",
        "    )\n",
        "\n",
        "def train_predictions(trainer, country, hidden_payload_json, true1, true2, true3):\n",
        "    global learner\n",
        "    if not hidden_payload_json:\n",
        "        return \"⚠️ No predictions yet.\", read_log_df()\n",
        "    try:\n",
        "        items = json.loads(hidden_payload_json)\n",
        "    except Exception:\n",
        "        return \"⚠️ State corrupted.\", read_log_df()\n",
        "\n",
        "    trues = [true1,true2,true3]\n",
        "    updated = 0\n",
        "    for itm, tcat in zip(items, trues):\n",
        "        if tcat and str(tcat).lower() in CAT2ID:\n",
        "            tid = CAT2ID[str(tcat).lower()]\n",
        "            learner.partial_fit([itm[\"sentence\"]],[tid])\n",
        "            corr_bin = corrected_binary(tcat)\n",
        "            ok_bin = (itm[\"pred_label\"]==corr_bin)\n",
        "            ok_cat = (itm[\"pred_cat_name\"].lower()==str(tcat).lower())\n",
        "            row = [\n",
        "                datetime.now().isoformat(),\n",
        "                trainer or \"\",\n",
        "                (itm.get(\"country\") or country or \"\"),\n",
        "                itm[\"question\"], itm[\"sentence\"], itm[\"model_source\"],\n",
        "                itm[\"bias_pct\"], itm[\"pred_label\"], itm[\"pred_cat_name\"],\n",
        "                f\"{itm['confidence']:.2f}\", (itm.get(\"rule_expl\") or \"\"),\n",
        "                tcat, corr_bin, bool(ok_bin), bool(ok_cat), itm[\"is_question\"], itm.get(\"method\",\"\")\n",
        "            ]\n",
        "            append_row(row); updated += 1\n",
        "    msg = f\"✅ Trained {updated} item(s).\" if updated else \"ℹ️ Nothing trained.\"\n",
        "    return msg, read_log_df()\n",
        "\n",
        "# ===================== Delete / Undo =====================\n",
        "def delete_row(row_index: float):\n",
        "    idx = int(row_index) if row_index is not None else -1\n",
        "    df = read_log_df()\n",
        "    if df.empty:\n",
        "        return \"ℹ️ Log is empty.\", df\n",
        "    if idx < 0 or idx >= len(df):\n",
        "        return f\"⚠️ Invalid row index: {idx}. Valid range: 0..{len(df)-1}\", df\n",
        "    df = df.drop(df.index[idx]).reset_index(drop=True)\n",
        "    df.to_csv(TRAIN_LOG_PATH, index=False)\n",
        "    global learner\n",
        "    learner = replay_from_csv()\n",
        "    return f\"🗑️ Deleted row {idx} and retrained model.\", df\n",
        "\n",
        "def show_history_with_index():\n",
        "    df = read_log_df().copy()\n",
        "    return df.reset_index().rename(columns={\"index\":\"row_index\"})\n",
        "\n",
        "# ===================== Metrics =====================\n",
        "SEVERITY_THRESHOLDS = {\n",
        "    \"Neutral\": (0, 0),\n",
        "    \"Low\": (1, 20),\n",
        "    \"Moderate\": (21, 40),\n",
        "    \"High\": (41, 70),\n",
        "    \"Critical\": (71, 100)\n",
        "}\n",
        "\n",
        "def bucket_severity(pct:int)->str:\n",
        "    if pct == 0: return \"Neutral\"\n",
        "    for name, (lo, hi) in SEVERITY_THRESHOLDS.items():\n",
        "        if name == \"Neutral\":  # already checked\n",
        "            continue\n",
        "        if lo <= pct <= hi:\n",
        "            return name\n",
        "    return \"Critical\"\n",
        "\n",
        "def metrics_from_items(items:list):\n",
        "    if not items:\n",
        "        return \"ℹ️ No predictions.\", pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "    df = pd.DataFrame(items)\n",
        "    if df.empty:\n",
        "        return \"ℹ️ No predictions.\", pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "    df[\"severity\"] = df[\"bias_pct\"].apply(bucket_severity)\n",
        "\n",
        "    by_model = (df.groupby(\"model_source\")[\"bias_pct\"]\n",
        "                  .mean().round(1).rename(\"% bias (mean)\").reset_index())\n",
        "\n",
        "    cat_counts = (df.pivot_table(index=\"model_source\", columns=\"pred_cat_name\",\n",
        "                                 values=\"bias_pct\", aggfunc=\"count\", fill_value=0))\n",
        "    cat_perc = (cat_counts.div(cat_counts.sum(axis=1), axis=0)*100).round(1)\n",
        "    dist_df = cat_counts.copy()\n",
        "    dist_df.columns = [f\"{c} (n)\" for c in dist_df.columns]\n",
        "    for c in cat_perc.columns:\n",
        "        dist_df[f\"{c} (%)\"] = cat_perc[c]\n",
        "\n",
        "    sev_counts = (df.pivot_table(index=\"model_source\", columns=\"severity\",\n",
        "                                 values=\"bias_pct\", aggfunc=\"count\", fill_value=0))\n",
        "    sev_perc = (sev_counts.div(sev_counts.sum(axis=1), axis=0)*100).round(1)\n",
        "    sev_df = sev_counts.copy()\n",
        "    sev_df.columns = [f\"{c} (n)\" for c in sev_df.columns]\n",
        "    for c in sev_perc.columns:\n",
        "        sev_df[f\"{c} (%)\"] = sev_perc[c]\n",
        "\n",
        "    header = []\n",
        "    for m in [\"ChatGPT\",\"Claude\",\"Gemini\"]:\n",
        "        if m in by_model[\"model_source\"].values:\n",
        "            val = float(by_model.loc[by_model[\"model_source\"]==m, \"% bias (mean)\"])\n",
        "            header.append(f\"{m}: {val:.1f}%\")\n",
        "    header_md = \"**Latest Run – Mean % Bias:** \" + \" | \".join(header) if header else \"ℹ️ No model scores.\"\n",
        "\n",
        "    return header_md, by_model, dist_df.reset_index(), sev_df.reset_index()\n",
        "\n",
        "def corpus_metrics(df: pd.DataFrame):\n",
        "    if df is None or df.empty:\n",
        "        return \"ℹ️ Log is empty.\", pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "    tmp = df.copy()\n",
        "    tmp[\"predicted_bias_pct\"] = pd.to_numeric(tmp.get(\"predicted_bias_pct\", 0), errors=\"coerce\").fillna(0).astype(int)\n",
        "    tmp[\"severity\"] = tmp[\"predicted_bias_pct\"].apply(bucket_severity)\n",
        "\n",
        "    by_model = (tmp.groupby(\"model_source\")[\"predicted_bias_pct\"]\n",
        "                  .mean().round(1).rename(\"% bias (mean)\").reset_index())\n",
        "\n",
        "    if \"predicted_category\" in tmp.columns and tmp[\"predicted_category\"].notna().any():\n",
        "        cat_counts = tmp.pivot_table(index=\"model_source\", columns=\"predicted_category\",\n",
        "                                     values=\"predicted_bias_pct\", aggfunc=\"count\", fill_value=0)\n",
        "    else:\n",
        "        cat_counts = pd.DataFrame(index=tmp[\"model_source\"].unique())\n",
        "\n",
        "    if not cat_counts.empty:\n",
        "        cat_perc = (cat_counts.div(cat_counts.sum(axis=1), axis=0)*100).round(1)\n",
        "        dist_df = cat_counts.copy()\n",
        "        dist_df.columns = [f\"{c} (n)\" for c in dist_df.columns]\n",
        "        for c in cat_perc.columns:\n",
        "            dist_df[f\"{c} (%)\"] = cat_perc[c]\n",
        "        dist_df = dist_df.reset_index()\n",
        "    else:\n",
        "        dist_df = pd.DataFrame()\n",
        "\n",
        "    sev_counts = tmp.pivot_table(index=\"model_source\", columns=\"severity\",\n",
        "                                 values=\"predicted_bias_pct\", aggfunc=\"count\", fill_value=0)\n",
        "    sev_perc = (sev_counts.div(sev_counts.sum(axis=1), axis=0)*100).round(1)\n",
        "    sev_df = sev_counts.copy()\n",
        "    sev_df.columns = [f\"{c} (n)\" for c in sev_df.columns]\n",
        "    for c in sev_perc.columns:\n",
        "        sev_df[f\"{c} (%)\"] = sev_perc[c]\n",
        "    sev_df = sev_df.reset_index()\n",
        "\n",
        "    # Optional accuracy (when true_category exists)\n",
        "    if \"true_category\" in tmp.columns and tmp[\"true_category\"].notna().any() and \"predicted_category\" in tmp.columns:\n",
        "        subset = tmp[tmp[\"true_category\"].astype(str).str.len() > 0].copy()\n",
        "        subset[\"correct_cat\"] = (subset[\"true_category\"].str.lower() == subset[\"predicted_category\"].str.lower())\n",
        "        overall_acc = (subset[\"correct_cat\"].mean()*100) if len(subset) else np.nan\n",
        "        by_model_acc = subset.groupby(\"model_source\")[\"correct_cat\"].mean().mul(100).round(1).rename(\"accuracy (%)\").reset_index()\n",
        "        accuracy_df = by_model_acc\n",
        "        header = \"### Corpus Metrics\\n\"\n",
        "        header += \"**Mean % Bias by Model:** \" + \" | \".join([f\"{r['model_source']}: {r['% bias (mean)']:.1f}%\"\n",
        "                                                             for _, r in by_model.iterrows()]) if not by_model.empty else \"N/A\"\n",
        "        if not np.isnan(overall_acc):\n",
        "            header += f\"\\n\\n**Category Accuracy (vs. true labels)**\\nOverall: {overall_acc:.1f}%\"\n",
        "    else:\n",
        "        accuracy_df = pd.DataFrame()\n",
        "        header = \"### Corpus Metrics\\n\"\n",
        "        header += \"**Mean % Bias by Model:** \" + \" | \".join([f\"{r['model_source']}: {r['% bias (mean)']:.1f}%\"\n",
        "                                                             for _, r in by_model.iterrows()]) if not by_model.empty else \"N/A\"\n",
        "    return header, by_model, dist_df, sev_df, accuracy_df\n",
        "\n",
        "# ===================================== Gradio UI =====================================\n",
        "custom_css = \"body { font-family: 'Times New Roman', Times, serif; }\"\n",
        "\n",
        "with gr.Blocks(css=custom_css, theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# 🧠 Hybrid Bias Detector (ML + Rule Fallback)\")\n",
        "\n",
        "    with gr.Tab(\"⚖️ Compare & Train\"):\n",
        "        trainer = gr.Dropdown(TEAM, label=\"Trainer\", value=None)\n",
        "        country = gr.Textbox(label=\"Country\", placeholder=\"e.g., Australia\")\n",
        "        question = gr.Textbox(label=\"Common Question\")\n",
        "\n",
        "        with gr.Row():\n",
        "            cgpt_ans = gr.Textbox(label=\"ChatGPT Answer\", lines=5)\n",
        "            cla_ans  = gr.Textbox(label=\"Claude Answer\", lines=5)\n",
        "            gem_ans  = gr.Textbox(label=\"Gemini Answer\", lines=5)\n",
        "\n",
        "        predict_btn = gr.Button(\"🔍 Run Predictions\", variant=\"primary\")\n",
        "        status_md = gr.Markdown()\n",
        "        sec_cgpt = gr.Markdown()\n",
        "        sec_cla  = gr.Markdown()\n",
        "        sec_gem  = gr.Markdown()\n",
        "        hidden_payload = gr.Textbox(visible=False)\n",
        "\n",
        "        t1 = gr.Dropdown(CATEGORIES, label=\"True Category: ChatGPT\")\n",
        "        t2 = gr.Dropdown(CATEGORIES, label=\"True Category: Claude\")\n",
        "        t3 = gr.Dropdown(CATEGORIES, label=\"True Category: Gemini\")\n",
        "        train_btn = gr.Button(\"✅ Train & Log\", variant=\"secondary\")\n",
        "        train_status = gr.Markdown()\n",
        "        recent_df = gr.Dataframe(headers=CSV_HEADERS, interactive=False, wrap=True)\n",
        "\n",
        "        predict_btn.click(\n",
        "            run_predictions,\n",
        "            inputs=[trainer, country, question, cgpt_ans, cla_ans, gem_ans],\n",
        "            outputs=[status_md, sec_cgpt, sec_cla, sec_gem, hidden_payload, t1, t2, t3]\n",
        "        )\n",
        "\n",
        "        train_btn.click(\n",
        "            train_predictions,\n",
        "            inputs=[trainer, country, hidden_payload, t1, t2, t3],\n",
        "            outputs=[train_status, recent_df]\n",
        "        )\n",
        "\n",
        "    with gr.Tab(\"📜 Full History\"):\n",
        "        full_hist = gr.Dataframe(headers=[\"row_index\"] + CSV_HEADERS, interactive=False, wrap=True)\n",
        "        refresh_btn = gr.Button(\"🔄 Refresh\")\n",
        "        full_hist.value = show_history_with_index()\n",
        "        refresh_btn.click(lambda: show_history_with_index(), outputs=full_hist)\n",
        "\n",
        "    with gr.Tab(\"🗑️ Delete Wrong Training\"):\n",
        "        gr.Markdown(\"Select the **row_index** from Full History and delete it if it was logged by mistake.\")\n",
        "        del_index = gr.Number(label=\"Row Index to Delete\", value=0, precision=0)\n",
        "        del_btn = gr.Button(\"🗑️ Delete Entry\", variant=\"stop\")\n",
        "        del_status = gr.Markdown()\n",
        "        del_hist = gr.Dataframe(headers=[\"row_index\"] + CSV_HEADERS, interactive=False, wrap=True)\n",
        "        del_btn.click(delete_row, inputs=del_index, outputs=[del_status, del_hist])\n",
        "\n",
        "    with gr.Tab(\"📂 Import / Export\"):\n",
        "        export_btn = gr.Button(\"📤 Download CSV\", variant=\"primary\")\n",
        "        export_file = gr.File()\n",
        "        upload_file = gr.File(file_types=[\".csv\"])\n",
        "        import_mode = gr.Radio([\"merge\",\"replace\"], value=\"merge\", label=\"Mode\")\n",
        "        import_btn = gr.Button(\"📥 Import CSV\")\n",
        "        import_status = gr.Markdown()\n",
        "        import_hist = gr.Dataframe(headers=CSV_HEADERS, interactive=False, wrap=True)\n",
        "\n",
        "        export_btn.click(export_csv_copy, outputs=export_file)\n",
        "        import_btn.click(\n",
        "            lambda f,m: (\"⚠️ No file.\" if f is None else import_csv(f.name,m)),\n",
        "            inputs=[upload_file,import_mode],\n",
        "            outputs=[import_status,import_hist]\n",
        "        )\n",
        "\n",
        "    # ======== 📊 Metrics Tab ========\n",
        "    with gr.Tab(\"📊 Metrics\"):\n",
        "        gr.Markdown(\"### Latest Run Metrics\\nUses the most recent predictions (above) without needing to train/log.\")\n",
        "        latest_btn = gr.Button(\"📈 Compute from Latest Run\")\n",
        "        latest_header = gr.Markdown()\n",
        "        latest_by_model = gr.Dataframe(label=\"Mean % Bias by Model\", interactive=False, wrap=True)\n",
        "        latest_cat_dist = gr.Dataframe(label=\"Category Distribution by Model\", interactive=False, wrap=True)\n",
        "        latest_sev_dist = gr.Dataframe(label=\"Severity Distribution by Model\", interactive=False, wrap=True)\n",
        "\n",
        "        def latest_metrics(hidden_json):\n",
        "            if not hidden_json:\n",
        "                return \"ℹ️ No predictions yet.\", pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
        "            try:\n",
        "                items = json.loads(hidden_json)\n",
        "            except Exception:\n",
        "                return \"⚠️ State corrupted.\", pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
        "            return metrics_from_items(items)\n",
        "\n",
        "        latest_btn.click(\n",
        "            latest_metrics,\n",
        "            inputs=[hidden_payload],\n",
        "            outputs=[latest_header, latest_by_model, latest_cat_dist, latest_sev_dist]\n",
        "        )\n",
        "\n",
        "        gr.Markdown(\"---\\n### Corpus Metrics (from CSV Log)\")\n",
        "        corpus_btn = gr.Button(\"🧮 Compute from Log\")\n",
        "        corpus_header = gr.Markdown()\n",
        "        corpus_by_model = gr.Dataframe(label=\"Mean % Bias by Model (Log)\", interactive=False, wrap=True)\n",
        "        corpus_cat_dist = gr.Dataframe(label=\"Category Distribution by Model (Log)\", interactive=False, wrap=True)\n",
        "        corpus_sev_dist = gr.Dataframe(label=\"Severity Distribution by Model (Log)\", interactive=False, wrap=True)\n",
        "        corpus_acc = gr.Dataframe(label=\"Category Accuracy by Model (if true labels present)\", interactive=False, wrap=True)\n",
        "\n",
        "        corpus_btn.click(\n",
        "            lambda: corpus_metrics(read_log_df()),\n",
        "            outputs=[corpus_header, corpus_by_model, corpus_cat_dist, corpus_sev_dist, corpus_acc]\n",
        "        )\n",
        "\n",
        "    # ✅ PCA tab — two plots (Category & Model)\n",
        "    with gr.Tab(\"🧩 PCA (Category & Model)\"):\n",
        "        gr.Markdown(\n",
        "            \"**Two PCA views**  \\n\"\n",
        "            \"• **Left:** colored by *category*  \\n\"\n",
        "            \"• **Right:** colored by *model* (point size/opacity = bias%)\"\n",
        "        )\n",
        "        dual_btn   = gr.Button(\"🖼️ Generate Both PCA Plots\")\n",
        "        img_left   = gr.Image(type=\"pil\", label=\"PCA — Category\", show_download_button=True)\n",
        "        img_right  = gr.Image(type=\"pil\", label=\"PCA — Model (Bias Encoded)\", show_download_button=True)\n",
        "        dual_status = gr.Markdown()\n",
        "\n",
        "        dual_btn.click(\n",
        "            fn=create_dual_pca_plots,\n",
        "            inputs=None,\n",
        "            outputs=[img_left, img_right, dual_status]\n",
        "        )\n",
        "\n",
        "demo.launch()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}